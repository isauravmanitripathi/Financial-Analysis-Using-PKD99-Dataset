{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will be challenged with a couple of analytical questions about the data. There might be no single correct answer to some of the questions, feel free to provide solutions which make the most sense to you. We know that you might not have the time to provide solutions to all the questions: try to finalize at least 3 of them and focus on the quality of your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to implement the solutions using Python and we would be most happy if you adopt PySpark for at least some of the exercises. However, you can pick the programming language of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/EntityRelation.png\"></src>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.appName(\"Scigility Test Challenge\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">In this part I work on loading the data, casting them in the correct format and removing unusable ones.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = ss.read.csv(\"dataset/trans.csv\",sep=';', inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def convertDate(x):\n",
    "    try:\n",
    "        cv_date = datetime.strptime(x, '%Y-%m-%d')\n",
    "    except:\n",
    "        cv_date = \"error\"\n",
    "    return cv_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_cast = df_trans.withColumn(\"date_2\", convertDate(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "1056320"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans_cast.filter(col(\"date_2\")==\"error\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">I choose to remove wrong data format (we can easily find again the wrong records).\n",
    "\n",
    "For the dates records, I will work with String only for a better display.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_clean = df_trans_cast.filter(col(\"date_2\")!=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_cast = df_trans_clean \\\n",
    "    .withColumn(\"trans_id\", df_trans_clean[\"trans_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account_id\", df_trans_clean[\"account_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"amount\", df_trans_clean[\"amount\"].cast(\"int\")) \\\n",
    "    .withColumn(\"balance\", df_trans_clean[\"balance\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account\", df_trans_clean[\"account\"].cast(\"int\")) \\\n",
    "    .drop(\"date_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan = ss.read.csv(\"dataset/loan.csv\",sep=';', inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_cast = df_loan \\\n",
    "    .withColumn(\"loan_id\", df_loan[\"loan_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account_id\", df_loan[\"account_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"amount\", df_loan[\"amount\"].cast(\"int\")) \\\n",
    "    .withColumn(\"duration\", df_loan[\"duration\"].cast(\"int\")) \\\n",
    "    .withColumn(\"payments\", df_loan[\"payments\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = ss.read.csv(\"data/order.csv\", sep=';', inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_cast = df_order \\\n",
    "    .withColumn(\"order_id\", df_order[\"order_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account_id\", df_order[\"account_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account_to\", df_order[\"account_to\"].cast(\"int\")) \\\n",
    "    .withColumn(\"amount\", df_order[\"amount\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_district = ss.read.csv(\"data/district.csv\", sep=';', inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_district_cast = df_district \\\n",
    "    .withColumn(\"district_id\", df_district[\"district_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A4\", df_district[\"A4\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A5\", df_district[\"A5\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A6\", df_district[\"A6\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A7\", df_district[\"A7\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A8\", df_district[\"A8\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A9\", df_district[\"A9\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A11\", df_district[\"A11\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A14\", df_district[\"A14\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A15\", df_district[\"A15\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A16\", df_district[\"A16\"].cast(\"int\")) \\\n",
    "    .withColumn(\"A10\", df_district[\"A10\"].cast(\"float\")) \\\n",
    "    .withColumn(\"A12\", df_district[\"A12\"].cast(\"float\")) \\\n",
    "    .withColumn(\"A13\", df_district[\"A13\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_district_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account = ss.read.csv(\"data/account.csv\", sep=';', inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_cast = df_account \\\n",
    "    .withColumn(\"account_id\", df_account[\"account_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"district_id\", df_account[\"district_id\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp = ss.read.csv(\"data/disp.csv\", sep=\";\", inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cast = df_disp \\\n",
    "    .withColumn(\"disp_id\", df_disp[\"disp_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"account_id\", df_disp[\"account_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"client_id\", df_disp[\"client_id\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = ss.read.csv(\"data/client.csv\", sep=\";\", inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_cast = df_client \\\n",
    "    .withColumn(\"client_id\", df_client[\"client_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"district_id\", df_client[\"district_id\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_cast.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card = ss.read.csv(\"data/card.csv\", sep=\";\", inferSchema=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card_cast = df_card \\\n",
    "    .withColumn(\"card_id\", df_card[\"card_id\"].cast(\"int\")) \\\n",
    "    .withColumn(\"disp_id\", df_card[\"disp_id\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card_cast.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some basic statistics of the data (mean, variance, etc.) of the “trans” table to understand it better. Create plots or visualizations of your choice (feel free to use the library you prefer. Hint: in case of Python, Matplotlib is probably best suited - in case of Scala you might want to use the Vegas library). Print and explain an aspect of your choice (that you think is interesting) in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Amount basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_cast.describe([\"amount\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">On average, transactions have an amount of 6K.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of transactions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_date = df_trans_cast \\\n",
    "    .groupBy('date') \\\n",
    "    .agg({'trans_id': 'count'}) \\\n",
    "    .select(\"date\",col(\"count(trans_id)\").alias(\"count\")) \\\n",
    "    .orderBy(df_trans_cast.date.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_date_pd = df_transaction_date.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot()\n",
    "myLocator = mticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(myLocator)\n",
    "plt.bar(df_transaction_date_pd['date'],df_transaction_date_pd['count'])\n",
    "plt.title(\"Total number of transactions per day\")\n",
    "plt.axvline(\"1997-06-01\", c='red')\n",
    "plt.axvline(\"1997-11-01\", c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">We note:\n",
    "\n",
    "- Number of transactions per day is increasing over time\n",
    "\n",
    "- There seems to have a significant number of transactions happening twice a year</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total amount of transactions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amount_date = df_trans_cast \\\n",
    "    .groupBy('date') \\\n",
    "    .agg({'amount': 'sum'}) \\\n",
    "    .select(\"date\",col(\"sum(amount)\").alias(\"sum\")) \\\n",
    "    .orderBy(df_trans_cast.date.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amount_date_pd = df_amount_date.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "ax = plt.subplot()\n",
    "ticks_y = mticker.FuncFormatter(lambda x, pos: int(x/1000000))\n",
    "ax.yaxis.set_major_formatter(ticks_y)\n",
    "myLocator = mticker.MaxNLocator(10)\n",
    "ax.xaxis.set_major_locator(myLocator)\n",
    "plt.bar(df_amount_date_pd['date'],df_amount_date_pd['sum'])\n",
    "plt.title(\"Total amount of transactions on time (in millions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">We note spikes happening regularly over time. We could thus think that the spikes are mostly caused by the number of transactions as seen previously.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the average loan amount per district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_for_ave = df_loan_cast.select(\"account_id\",\"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_for_ave = df_account_cast.select(\"account_id\",\"district_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_district = df_loan_for_ave \\\n",
    "    .join(df_account_for_ave, df_loan_for_ave.account_id == df_account_for_ave.account_id, 'left') \\\n",
    "    .drop(df_loan_for_ave.account_id) \\\n",
    "    .drop(df_account_for_ave.account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_district_gp = df_loan_district \\\n",
    "    .groupBy('district_id') \\\n",
    "    .agg({'amount': 'avg'}) \\\n",
    "    .select(\"district_id\",col(\"avg(amount)\").alias(\"avg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_district_gp.sort(col(\"avg\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_district_pd = df_loan_district_gp.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(df_loan_district_pd['district_id'],df_loan_district_pd[\"avg\"])\n",
    "plt.title(\"Loan amount per district\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_district_gp.describe(\"avg\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Loans are between 74K and 294K across all districts.\n",
    "\n",
    "District 46 has the highest amount.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit risk prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an ML model that classifies if a certain loan will be paid or not. You can use any classification model you think is suitable (hint: if you’re using Spark, there are some available out of the box with MLlib - with Python and Scikit-Learn too). Note that the goal is not to get the accuracy as high as possible – it is completely OK to choose a simple model and not spend days on parameter tuning. Think about questions like:\n",
    "\n",
    "- Which model do you choose and why?\n",
    "\n",
    "- How do you do the training and how do you measure the model accuracy?\n",
    "\n",
    "- Which are the variables contributing the most to the prediction?\n",
    "\n",
    "- How accurate does your model get? If you think a higher accuracy is possible, what would be the next steps you take?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">In this section I conducted simple analysis to understand better the data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From data provider https://sorry.vse.cz/~berka/challenge/pkdd1999/berka.htm :\n",
    "\n",
    "'A' stands for contract finished, no problems,\n",
    "\n",
    "'B' stands for contract finished, loan not payed,\n",
    "\n",
    "'C' stands for running contract, OK so far,\n",
    "\n",
    "'D' stands for running contract, client in debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_cast.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_cast.filter(col(\"status\")==\"D\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_for_join = df_loan_cast.select(\"loan_id\",\"account_id\",\"amount\",\"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_list = ['A', 'B', 'C', 'D']\n",
    "count_status = []\n",
    "for status in status_list:\n",
    "    count_status.append(df_loan_cast.filter(col(\"status\")==status).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.pie(count_status, labels=status_list, autopct='%0.0f%%')\n",
    "plt.legend()\n",
    "plt.title('status of loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Most clients have a running contract that is OK so far.\n",
    "\n",
    "Among the sample, 7+5=12% of the loans are missing a payment.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "In order to predict whether a loan will be paid or not in a relevant manner, I choose to focus on the following features:\n",
    "    \n",
    "- **date** when the loan was granted\n",
    "\n",
    "- **amount** of money\n",
    "\n",
    "- **duration** of the loan\n",
    "\n",
    "- **type** of card\n",
    "    \n",
    "**Status** of paying off the loan will be used as the value to predict.\n",
    "    \n",
    "In this section I focus on operation to prepare the data for machine learning algorithms, that is:\n",
    "    \n",
    "- join\n",
    "    \n",
    "- conversion \n",
    "    \n",
    "- standardization\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_for_join = df_disp_cast.select(\"disp_id\", \"account_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_disp = df_loan_cast \\\n",
    "    .join(df_disp_for_join, df_loan_cast.account_id == df_disp_for_join.account_id, 'left') \\\n",
    "    .drop(df_disp_for_join.account_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Note: since an account can have several clients (and thus several disp_id), the number of records increase. I think it makes sense to consider different records for different clients.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_card_for_join = df_card_cast.select(\"disp_id\",\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loan_disp_type = df_loan_disp \\\n",
    "    .join(df_card_for_join, df_loan_disp.disp_id == df_card_for_join.disp_id, 'left') \\\n",
    "    .drop(df_card_for_join.disp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def typeToInt(x):\n",
    "    if(x==\"junior\"):\n",
    "        n = 0\n",
    "    elif(x==\"classic\"):\n",
    "        n = 1\n",
    "    elif(x==\"gold\"):\n",
    "        n = 2\n",
    "    else:\n",
    "        n = -1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def statusToInt(x):\n",
    "    if(x==\"A\"):\n",
    "        n = 1\n",
    "    elif(x==\"B\"):\n",
    "        n = 2\n",
    "    elif(x==\"C\"):\n",
    "        n = 3\n",
    "    elif(x==\"D\"):\n",
    "        n = 4\n",
    "    else:\n",
    "        n = -1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "# Convert to number\n",
    "df_type_int = df_loan_disp_type \\\n",
    "    .withColumn(\"type_2\", typeToInt(\"type\")) \\\n",
    "    .withColumn(\"status_2\", statusToInt(\"status\")) \\\n",
    "    .withColumn(\"date_2\", unix_timestamp('date', 'yyy-MM-dd'))\n",
    "\n",
    "# Change column format\n",
    "df_type_cast = df_type_int \\\n",
    "    .withColumn(\"status_int\", df_type_int[\"status_2\"].cast(\"int\")) \\\n",
    "    .withColumn(\"type_int\", df_type_int[\"type_2\"].cast(\"int\")) \\\n",
    "    .withColumn(\"date_int\", df_type_int[\"date_2\"].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_na = df_type_cast \\\n",
    "    .withColumn('type_na', when(df_type_cast.type.isNull(),lit('undefined')).otherwise(df_type_cast.type)) \\\n",
    "    .drop(\"type\",\"type_2\", \"status_2\", \"date_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_scaled = df_type_na\n",
    "\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType()) # convert column type from vector to double type\n",
    "\n",
    "for colName in [\"amount\", \"type_int\", \"date_int\", \"duration\"]:\n",
    "    assembler = VectorAssembler(inputCols=[colName],outputCol=colName+\"_vect\") # convert to vector type\n",
    "\n",
    "    scaler = MinMaxScaler(inputCol=colName+\"_vect\", outputCol=colName+\"_scaled\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    df_type_scaled = pipeline \\\n",
    "        .fit(df_type_scaled) \\\n",
    "        .transform(df_type_scaled) \\\n",
    "        .withColumn(colName+\"_scaled\", unlist(colName+\"_scaled\")) \\\n",
    "        .drop(colName+\"_vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_scaled_pd = df_type_scaled.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_scaled_pd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groups by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(samples, labels, dico_labels, features=[0,1], feature_names=None, display_labels=True):\n",
    "    '''Display the samples in 2D'''\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    if display_labels:\n",
    "        nb_labels = np.max(labels)\n",
    "        for j in range(nb_labels + 1):\n",
    "            nb_samples = np.sum(labels == j)\n",
    "            if nb_samples:\n",
    "                index = np.where(labels == j)[0]\n",
    "                plt.scatter(samples[index,features[0]],samples[index,features[1]],s=60, label=d[j])\n",
    "    else:\n",
    "        plt.scatter(samples[:,features[0]],samples[:,features[1]],color='gray')\n",
    "    if feature_names is not None:\n",
    "        plt.xlabel(feature_names[0])\n",
    "        plt.ylabel(feature_names[1])\n",
    "    plt.legend(loc=(0.9,0.3))\n",
    "    plt.axis('auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = ['amount','type']\n",
    "d = {1:\"A\", 2:\"B\", 3:\"C\", 4:\"D\"}\n",
    "show_samples(samples=np.array(df_type_scaled_pd[[\"amount_scaled\",\"type_na\"]]), \\\n",
    "             labels=np.array(df_type_scaled_pd[\"status_int\"]), dico_labels=d, \\\n",
    "             display_labels=True, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">\n",
    "\n",
    "We note:\n",
    "\n",
    "- Most of the clients who cannot paid their loans do not have a card\n",
    "\n",
    "- A large proportion of contracts that finished without any issue are for lower amounts\n",
    "\n",
    "- No junior client are in debt for paying a contract\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">It confirms that using the type as a feature is relevant.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction for 2D display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2, whiten = True)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">The dimension reduction allows us to still catch 72% of the variance.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(X_reduced[:,0],X_reduced[:,1], '+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">We can see that the data are largely clusterizable.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Multiclass approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">I chose to start with one of the easiest algorithm; it's also the model I know the best so I am able to better extract information of it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sm = sm.OLS(Y,X2)\n",
    "results = lr_sm.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">\n",
    "\n",
    "- Relationship is highly significant globally since p-value associated with Fisher stat is very low\n",
    "\n",
    "- All variables are significant expect the amount\n",
    "\n",
    "- **Date** and **Duration** are the most important factors\n",
    "\n",
    "- Type is actually not that important (contrary to what we expected in previous part)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LinearRegression()\n",
    "clf_lr = clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">First results seem to be satisfying since we want at least a result > 50%.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">Let's run a cross validation to check whether results are stable with data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf_lr, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">Results seem volatile but still satisfying.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">The decision tree is a simple algorithm for non linear relations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tr = tree.DecisionTreeClassifier(random_state = 0)\n",
    "clf_tr = clf_tr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_tr.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">Result is quite high.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tr.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">**Date** and **Duration** are the most important features => in line with the Linear Regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.export_graphviz(clf_tr, out_file=\"TreeScigility.dot\", filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Picture displayed using http://viz-js.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/Tree.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">The depth is too high to have a good interpretability.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost (Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Boosting methods may be well adapted with such few data as we won't be penalised by the computational time.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_adb = AdaBoostClassifier()\n",
    "clf_adb.fit(X_train, Y_train)\n",
    "clf_adb.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">KNN is well adapted for our case since we were able to identify clusters in the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_int\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kn = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_kn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kn.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_scaled_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_scaled_pd['status_bin'] = np.where(df_type_scaled_pd['status'] == \"D\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lg = LogisticRegression()\n",
    "clf_lg = clf_lg.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lg.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">Results are even better than in multiclass prediction.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_lg = clf_lg.predict_proba(X_test)\n",
    "pred_lg = prob_lg[:,1]\n",
    "fpr_lg, tpr_lg, _ = metrics.roc_curve(Y_test, pred_lg)\n",
    "roc_auc_lg = metrics.auc(fpr_lg, tpr_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tr = tree.DecisionTreeClassifier()\n",
    "clf_tr = clf.fit(X_train,Y_train)\n",
    "clf_tr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tr = clf_tr.predict_proba(X_test)\n",
    "pred_tr = prob_tr[:,1]\n",
    "fpr_tr, tpr_tr, _ = metrics.roc_curve(Y_test, pred_tr)\n",
    "roc_auc_tr = metrics.auc(fpr_tr, tpr_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {\n",
    "            'n_estimators':[1,10,20,50,100],\n",
    "            'learning_rate':[0.1,0.2,0.3,0.5,0.8]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid = GridSearchCV(estimator = AdaBoostClassifier(), param_grid = param_test, scoring='roc_auc', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_grid.best_score_)\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ab = AdaBoostClassifier(n_estimators=100, learning_rate=0.3)\n",
    "clf_ab = clf_ab.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_ab = clf_ab.predict_proba(X_test)\n",
    "pred_ab = prob_ab[:,1]\n",
    "fpr_ab, tpr_ab, _ = metrics.roc_curve(Y_test, pred_ab)\n",
    "roc_auc_ab = metrics.auc(fpr_ab, tpr_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_type_scaled_pd[[\"amount_scaled\",\"date_int_scaled\",\"duration_scaled\",\"type_int_scaled\"]]\n",
    "Y = df_type_scaled_pd[\"status_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kn = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_kn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_kn.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"green\">Still very high score.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_kn = clf_kn.predict_proba(X_test)\n",
    "pred_kn = prob_kn[:,1]\n",
    "fpr_kn, tpr_kn, _ = metrics.roc_curve(Y_test, pred_kn)\n",
    "roc_auc_kn = metrics.auc(fpr_kn, tpr_kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_models = {'log_reg':(fpr_lg, tpr_lg, roc_auc_lg),'tree':(fpr_tr, tpr_tr, roc_auc_tr), \\\n",
    "                 'adaboost':(fpr_ab, tpr_ab, roc_auc_ab), 'knn':(fpr_kn, tpr_kn, roc_auc_kn)}\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for model in metric_models:\n",
    "    fpr, tpr, roc_auc = metric_models[model]\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = model + ' AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "Those predictions can lead to 2 types of errors:\n",
    "\n",
    "- False positive: the model wrongly predicted that the client will pay its loan\n",
    "\n",
    "- False negative: the model wrongly predicted that the client won't pay its loan\n",
    "\n",
    "A bank would probably want to make sure the loans are indeed paid. They will thus be in favor of an algorithm that minize the first error.\n",
    "\n",
    "Thus, AdaBoost seems the best algorithm.\n",
    "\n",
    "**Limits**: explainability, computational time with more data.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
